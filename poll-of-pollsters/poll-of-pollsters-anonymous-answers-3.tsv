"How do you determine how likely one of your respondents is to vote? Do you weight them, or is each respondent either a likely voter or not one? Please provide as much detail as possible."	"Do you weight by party affiliation? If so, how do you determine what weights to use? Please provide as much detail as possible."	Do you ever poll from registered voter lists rather than call at random?	Why?	"When you weight poll results, is there a maximum weight you use to increase the count of a demographic subgroup?"	"Why or why not, and if yes, what is that weight?"	Do you weight by race and party together? (Example: weighting African-American Democrats instead of African-Americans and Democrats separately.)	"In what circumstances do you do so, and why?"	Do you ever deliberately call back prior poll takers [http://fivethirtyeight.com/features/oct-16-can-polls-exaggerate-bounces/]?	"If you do sometimes or always do that, under which circumstances, how do you do so, and why? If not, why not?"	Do you have off-the-record conversations with other pollsters to compare results before publishing them?	Why or why not?	Do you use Bayesian methods or frequentist methods?		Why?	Do you find traditional reporting of statistical margin of error to be credible?	"If so, why? And if not, how do you think margin of error should be reported?"	In what languages other than English do you ever ask your political polls?											What percentage does it add to the cost of a poll to add one language?	"If you field polls in Spanish, how do Hispanic respondents who choose to answer in Spanish differ from those who answer in English?"	"How much does it cost you to poll one stateÍs Senate race, on average?"	"How much did it cost you to poll one stateÍs Senate race, on average, in 2010?"	"How do you account for the change, if any?"	Do you always disclose who is funding your polls?	Why or why not?	Do you ever conduct and publish political polls without sponsors?	"If not, why not and would you ever do so? If so, under what circumstances and why?"	How many total employees (full-time or part-time) does your polling organization have?	How many are men?	How many are women?	How many are white?	How many are African-American?	How many are Hispanic?	How many are Asian American?	Any comments on the demographics of your staff?	Do you ever poll using an online panel?	Do you cap the number of polls that panel members can take in a given time period?	"Why or why not? And if you do, at what level do you cap it?"	What percentage of your panel members leave or become inactive annually?	Any comments on your panel turnover?	Do you ever poll by phone?	Have decreasing response rates required you to change your techniques by using increased weighting or supplementing with different technologies?	Please elaborate on your answer above.	Do you ever poll by phone using live interviewers?	How much do you pay your interviewers per hour?	What percentage of your interviewers are male?	What is your interviewers' average age?	How many hours of training do you require them to have before they can conduct interviews?	How are your interviewers trained to handle invective from people who hate being called?	What percentage of interviews do you monitor for quality?	Do you ever interview by phone using Interactive Voice Response (IVR)?	For what percentage of IVR polls do you use male voices?	For what percentage of IVR polls do you use female voices?	"Whose voices do you use? i.e. actors, local TV personalities?"	"How much do you pay the people whose voices you record, by poll or by hour?"	"How many seats do you expect Republicans will control in the Senate in 2015? (Yes, we're asking again.)"	Why?	What question or questions would you want us to ask your fellow pollsters in future rounds of this poll?Open-Ended Response	Open-Ended Response	Response	Open-Ended Response	Response	Open-Ended Response	Response	Open-Ended Response	Response	Open-Ended Response	Response	Open-Ended Response	Response	Sometimes or Other (please specify)	Open-Ended Response	Response	Open-Ended Response	Spanish	Mandarin or other Chinese	Tagalog	French	Vietnamese	German	Korean	Russian	Arabic	None	Other (please specify)	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Response	Open-Ended Response	Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Response	Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Response	Response	Open-Ended Response	Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response	Open-Ended Response"Cut-off. Since I use RBS [registration-based sampling], the sample is pre-screened by past voting history before I even dial with an additonal cut based on self-reported likelihood."	No.	No		No	But we look carefully at our weights.	No	I do not weight by party ID.	No	It's simply not the type of work we do.	No	"Don't want to drive myself crazy. Also, on congressional districts and state senate districts, often no other public pollster is doing them."	Bayesian		We use Bayesian because it allows us to change our models as more information becomes available.	No	"For phone polling it's fine. We poll 100% online, so we provide a comparison-purposes only example of what an MoE on a statistical sample the same size would yield. There is a much wider debate at play regarding the reliability of phone versus online polling and the MoE is symptomatic of that."	Spanish	Mandarin or other Chinese	Tagalog						Arabic				Less acculturated Hispanics tend to be more Democratic.	"$2,200 "	"$1,750 "		No	"For publicly released polls, yes. For private clients who request anonymity, no."	No	Funding constraints would prohibit us from conducting such polls.	2	0	2	2	0	0	0		No			It's not our proprietary panel; we don't have access to this information.		Yes	No	Only been polling since 2012.	Yes	$27/hour	55%	22	20+ hours	They are required to add them to our do-not-call list and told to never be impolite.	varies	Yes	0%	100%	Our main voice is simply someone who has a great phone voice and good instincts about how to read poll questions. We have also used people with a media background in special situations.	$25/hr	50	"Alaska seems unknowable. Colorado still seems hard to believe (I'm probably unskewing that one, sad to say). And who knows about Georgia?"	"Academic pollsters -- Do you pay your callers minimum wage? Do you withhold taxes, Social Security & Medicare? Do you match Social Security & Medicare, or do you consider your callers ""independent contractors"" and force them to pay 100%? Does your institution include coverage for callers in its Worker's Compensation Insurance policy?"Each respondent is either a likely voter or not.	No. I weight by party registration from the RBS [registration-based sample] list. Turnout by party registration is remarkably stable -- e.g. there was very little shift on that demographic from the 2006 Democratic year to the 2010 Republican year -- whereas self-reported party ID per the exit polls did change. (I have only polled states with party registration).	Yes	"Have done both in past. Now using RBS [registration-based sampling] exclusively for midterms and primaries. Have yet to decide about 2016 general. Note: For other than ""likely voter"" polls (i.e. the policy and quality of life polling that make up the vast majority of our output -- an area FiveThirtyEight isn't as interested in) we still use RDD [random-digit dialing], since those findings are based on the general population"	No	"We may trim weights as dictated by the makeup of each sample, if necessary. But we do not have a specific, static cut-off that we apply across samples."	No		No	We believe in the true randomization method.	No	"Our results are processed quickly and even were we inclined to, we could not do so. And we are not inclined to."	Bayesian			No	"Margin of sampling error should not be reported. It's misleading, it's not applicable in 2014, and it mis-informs poll consumers as to the real sources of error that drive differences across polls and pollsters."	Spanish											10% to 15%		"$4,425, approximtaely, if in English only"	"$4,410 "	Need more sample to offset lower response rate(s).	Yes	"99% of our ""public"" polls are paid for by us. If someone else pays, we are up front about it."	No	We always have a sponsor as our company is owned in part by the owners of national newspaper chain and the company owns its own news sites.	2	2	0	2	0	0	0		Yes	Sometimes	"Sometimes is not a good answer. We did a study of five data-collection methods using the same questionnaire in the same market at the same time. One method was a panel, so I've done it. Even though I told the company we would be comparing the quality of data across methods, the panel was pretty bad. That's the mail panel. The online panel had to be redone because it was managed so poorly. I'm not a fan."		Our retention rates are very strong.	Yes	No	"Same techniques -- weighting is routine now, though it used to be occasionally needed, in the olden days."	Yes	Again I don't have responses to all these questions -- it would take a while to get the answers from our phone unit (a separate business unit).					"15%-20%+, depending on project and client requirements"	Yes	50%	50%	Local voice talent	By poll: $50 for some; $75 for longer ones.	50	We aren't as convinced as others about the alleged GOP wave.	Ask why pollsters use telephone? Ask why they use IVR [interactive voice response]? Ask why they use online?Focus on likely electorate using data in voter file on past participation.	"Not by party self-ID. In states with party registration, only if the sample is way out of whack with actual voter-registration statistics."	Yes	"In congressional districts and state senate, as RDD [random-digit dialing] error is overwhelming in those geographies."	No		No		No		No	"The only purpose in having such conversations would be to change your numbers to be closer to the results of others. We don't change our results to match or come closer to others. You would be changing the data, and that is just plain wrong."	Bayesian			No	"There's nothing wrong with including the standard MOE in a report (as long as the phrase ""statistical dead heat"" doesn't rear its ugly head). But a total survey error approach is much more legitimate at this point. Also, the general public can't really consume effect sizes and p-values. But it would be much more informative if those were reported as well."	Spanish											2x	No consistently observable difference.	"$8,000 (if part of larger polling program so we have a lot of infrastructure already set up)"	"$10,000 (ditto caveats above)"	"Use of nonpanel sample means large, geographically targeted base sizes are easier to achieve online than they used to be."	Yes	AAPOR [American Association for Public Opinion Research]	No	"What does sponsorship mean? We have media partners that we work with on our polls, but they do not provide any financial support for the polls."	5	3	2	4	0	1	0	N/A	Yes	Sometimes	This is something that is new for us. We have emails on 30% of all voters.	I really don't know but we can request that data for you	"No, but again we can probably get that data from our project staff or vendor."	Yes	No	"We have used online surveys more often for specific types of surveys, but not for our political surveys. We have used screens to make sure we have adequate representation of younger age categories, and we have also increased the proportion of households with younger voters to increase the chances of including younger voters."	Yes	Between $8.50 and $14.75 per hour						Yes	50%	50%	Professiona voice and robotics	N/A	51	Because that's what the polls are showing!	How do they get cellphone numbers -- is it RDD [random-digit dialing] or RVV? Do they own IVR [interactive voice response] software? Do they own the call center that they make the calls with? How do they handle calling cellphones in the call center since you can't use a predictive dialer?"Generally, our likely-voter screens are in the tradition of the Gallup screens, which have historically used a series of questions to identify likely voters. When clients request it, we can use a system that gives each respondent a weight representing their relative likelihood to vote."	"Only when necessary to have the party affiliation within a range that is determined by examining our own polling data from the previous election cycle for governor or president that best applies to the upcoming election, looking at an average of polls for previous similar elections that were close to the election outcome, but most importantly, calculating the base partisan vote for statewide education posts as a percentage of the total vote for governor or president, whichever applies to the upcoming election. There has been great consistency in our party affiliation for most polls, and with rare exceptions, we have only had to weight party affiliation by 2 or 3 points, and any party reweighting has only been requred about a third of the time."	Yes	"In election polling, we do this almost exclusively. (Although I should clarify that they are still random samples, just drawn from a more restricted sampling frame.) First, we believe that it simply doesn't make much sense to include people who can't vote in the frame parameters when trying to assess an electoral outcome. We wouldn't include Canadians, for instance, so why would we include non-registered Americans!? Second, voter lists usually provide more information about respondents (like voter-propensity scores, party propensity, etc.) than you'd get from an RDD [random-digit dialing] sample. Even if we don't generally use these scores for weighting, they're valuable to use as validity checks for the data. Third, many major campaign pollsters also seem to favor voter-list-based polling, and have based this decision on empirical evidence they have gathered."	No		No		No		No	"We do most of our polling in races where there are few or no other polls. But even when we do work in more heavily polled races, I wouldn't want my judgment influenced by somebody else's choices. I do discuss polls with other pollsters after the fact, however."	Bayesian			No		Spanish											c. 10%	"NOTE that our main polling is asked in English only. But other smaller polls we have run, such as polling for newspapers in Florida, incorporate Spanish. As you would expect for acculturated vs. unacculturated groups... there is a lot of variation in demographics and political attitudes."	"Automated: about $500. Live interview: about $2,000."	About $200.	Cost of contacting cellphones.	Yes	"If it is published, we usually provide it to our media clients as an exclusive before other or all media outlets, and we always disclose who funded the poll if it was not commissioned by our media clients."	No		5	4	1	5	0	0	0	"The larger staff on the election-management side is highly diverse in terms of sex, orientation, race/ethnicity, age, education level, etc. The much smaller polling operation evolved from that, based on people's specific training, skill sets, interest, and availability. Based on those parameters, it just happened to be a much less diverse subgroup of the larger organization."	Yes	Sometimes	"We cap the number of OUR polls that a panelist can complete in a given period of time, but we do not cap the number of other polls that a panelist might participate in."			Yes	No	"We supplement with cells, but because of the movement away from landlines, not because of decreasing response rates."	Yes	"I pay call centers about $22 per hour, all inclusive."	Not sure	Not sure	Depends upon the call center -- but they are established firms that I am sure all have good training & monitoring in place.	Not sure	We remote call monitor about 15%	Yes	about 20%	about 80%	"TV news anchors combined with professional ""voice-over"" talent"	We pay the talent annually.	51	Just a guess.	"How many hours did it take you to complete this ""20 minute"" survey? (That's not really a suggested question, obviously!)""It depends on the type of election, how close the poll is to Election Day, and whether there are voter propensity scores available on the particular voter file. So, there's no single formula. But typically, we first apply a liberal likely-voter screen when drawing a sample. For example, in state-level Senate & gubernatorial polls we have conducted within the past two weeks, we have selected our random samples from among registered voters who have voted at least once in the past three even-year elections (2008, 2010, or 2012), or first registered in December 2012 or more recently. We have experimented with including (registered) individuals with no recent history of voting, but their response rate tends to be negligible, and those who do respond tend to select ""undecided"" in matchup questions at a very high rate. This effect may be heightened in IVR [interactive voice response] polling (which is the method we use nearly exclusively), but I don't have live-caller numbers with which to compare. Our second step is a self-rated voter-likelihood question. When we are more than a few weeks out from an election, we usually use that question simply as a crosstab, rather than a weighting variable. But in a poll close to an election, particularly when early voting is underway, we will use the likelihood question as one of our weighting factors, advantaging people who say they have already voted, and reducing the impact of those who say they're not yet sure when or how they're voting (if they say they're NOT voting, they are typically dropped). We have found that self-identified early voters tend to track with final results pretty closely. On the other hand, those who aren't sure when or where they're voting also tend to have high ""undecided"" numbers, so weighting them down affects the undecided number much more than the head-to-head candidate numbers. Finally, in a general-election poll just before an election, we may also apply a voter propensity score adjustment (we wouldn't do this in a primary or a local election, because nearly everyone in those samples is likely to have a very high propensity score, and there's very little variability in those scores). However, applying a propensity adjustment often makes the weights more extreme for a small number of respondents (i.e., high-propensity individuals from low-propensity demographic groups) without changing the topline result in any appreciable way. In such cases, we tend to stick with our demographically adjusted results without adding propensity scores to the mix."	"There is not ""party registration"" in every geography in which we poll, so it's impossible to do any one thing consistently and uniformly."	Yes	It is impossible to do a RDD [random-digit dialing] poll in a congressional district.	No		No		No		No		Frequentist		"It's what I was trained to do in graduate school. And it seems to work well in polling. (Ironically, it probably doesn't work as well in psychology, which is the field in which I was trained.) We don't, however, go out of our way to stress the binary nature often associated with frequentist methods (i.e, is the result within or outside the MOE [margin of error]?) -- I prefer to opt for an effect-size approach over a significance-testing approach to the degree possible."	Yes	"A poll uses Bayesian sampling techniques, then the margin of error should be reliable."	Spanish											Depends on the language and the proportion. It doubles the cost on an individual interview.		Depends on the sample size	Depends on the sample size		Yes	"If the polls are released to the public. One exception is that if we choose to piggy-back some political questions for our own use on to a private poll. If we decide to release the political findings, we do not identify the client whose poll we added them to. That's primarily a contractual requirement."	Yes	"At times in the past, we have conducted a poll on a congressional or state legislative race that is hotly contested -- if there has not been polling done, if we question the results of other polls that were published, or if we are just curious about an election."	8	7	1	7	0	1	0	Irrelevant.	Yes	Yes	Good luck with that!			Yes	No	"We've had to call more people, but the techniques remain the same."	Yes	I use an independent call center. I pay by the project.						Yes	Rarely. And not for election polls	100%	0	Can't remember.	51	"Looks like the terrain is just too tough for Democrats to hold on to their majority, but enough races are close that Democrats should be able to pull out a couple of the close ones, which would prevent the GOP from getting a bigger majority."	Why the fuck they stay in this god-awful businessSeveral screener questions: 1) Are they registered; 2) Have they already voted absentee or voted early; 3) Five-point scale -- keep top two.	"We do NOT weight by self-described party affiliation, but we do weight on voting history based on declared party affiliation in primaries."	Yes	More accurate; more cost-effective.	No		No		Sometimes	For anecdotal information	No		Frequentist			Yes	"By pollsters, not by journalists. And would prefer all report design effect so as to add the impact of sampling error to the MoE."	Spanish											Depends upon the percentage of Hispanic voters in the electorate.	I haven't noticed a consistent pattern.	"Depends upon the sample size and questionnaire length, etc."	"About 20%, less on average."	"Primarily, the need to include an increasing number of cellphone interviews."	Yes	Obligatory.	Yes	We conduct public polls for publicity.	10	4	6	9	0	0	1		Yes	Yes	We subscribe to a company that provides fresh panels for each survey.			Yes	No		Yes	"I work with a vendor, so this information is not at my command"						Yes	We use the same male voice for IVR calls because of the quality of his voice.		No	"It is provided as part of the cost of the project, usually without cost."	51		"Use a version of Pew's series -- 4 questions on likely, knowledge of election, politics and candidates -- which yields a score. We, as election approaches, keep only those with a progressively higher score. Then at the end, we ask surety to vote 1 to 10, so we have a front and back score."	"We weight by party affiliation. We usually conduct completely separate surveys with only the question of, ""What is your political affiliation?"" and then the usual demographic questions. In states where there is no registration by party, this has proved to be essential."	Yes	"That is the way we have always done it and it has been successful for us, not only for our statewide polls for our media clients, but also for polls we do for our other clients when they are having us test ballot proposals, such as school districts, community colleges, counties, local governments, libraries and transit authorities. Also, since we maintain a voter file with complete vote history, which includes new registrants."	Yes	10%-15%	No		Yes	For panel design  or reusing special groups at a later time.	No		Frequentist			Yes	"It's credible as far as it goes. The problem is journalists are crap with numbers so they don't really know how to interpret it. A lead is a lead, Not a 'statistical tie."" And in many cases questionnaire design may introduce more error than sampling."	Spanish											Depends.	Yes	I'm the methodologist. I actually don't have this information.			Yes	Transparency	Yes	"We do our own, as such, we are the sponsor."	10	7	3	8	1	1	0		Yes	Yes				Yes	No		Yes	"Since we use contractors, I don't have this information readily available."						Yes					51		"We have a database of voters with vote history. Each sample that we pull includes households with any general-election vote history of household members, or households with new registrants. Our screen asks if the respondent voted in either of the past two general elections, or both, and if the respondent voted in neither, they are asked if the reason they did not vote in either election is because they were too young or not registered to vote at the time of the election, in which case all respondents who qualify continue and are asked their intent to vote in the upcoming election. If they are very certain, somewhat certain, or will likely vote, they continue. If not, their interview is terminated."	"Where there is party registration, weight to best estimate of registration in likely electorate  weight party ID to rolling average of our polls."	Yes	There is a difference in taking a random sample of registered voters from a source such as Aristotle [http://aristotle.com/] and then randomly dialing from that list. Most of our colleagues whom we normally see polling in our area do the same.	Yes	"For the majority of our surveys, the weights are trimmed if they exceed a certain value, which can vary survey to survey."	No		Yes	"Only with the respondent's pre-permission, and only under idiosyncratic circumstances. Not systemically."	Yes	We often get called by others to compare notes since we're one of the few willing to publish results.	Frequentist			Yes	"It's good enough given that the average media audience has no clue how to correctly interpret the ""horsey-ducky"" version."	Spanish											Double	"They really donÍt differ from Hispanics who take polls in English; we do get more responses from the bloc in Spanish, though."	N/A	unknown	N/A	Yes	We do not poll for partisan organizations or campaigns.	Yes	We have only done this a couple of times. We did it to test out some methodological adjustments and to increase our public exposure.	14	8	6	12	1	0	1	It changes.	Yes	Yes				Yes	Yes	More weighting and switch to voter file.	Yes	"We contract for our live interviews, but I believe they are paid above minimum wage, depending on experience."	"Unknown, since we contract for the service."	Unknown	"Unknown, but there are rarely any problems."	"We stress the value of their opinion in a variety of tactful ways, but if they are upset, we just tell them they will be removed from our calling list, which we do."	25%	Yes					52	Because Mark Pryor [Arkansas Democratic candidate] and Bruce Braley [Iowa Democratic candidate] ran shitty campaigns and the map was a bad-luck draw for Democrats.	"We have experimented with likely-voter screens that contain as many as six questions and as few as one question. There is no simple relationship between the number of screening questions and the accuracy of the final vote estimate. In 2014, we are using a single question which offers respondents a range of options from ""absolutely certain"" you will vote to ""absolutely certain"" you will not vote."	"Yes, based on prior elections."	Yes	Use voter-registration list matched to cellphone numbers to get cell voters in statewide polls. Random-dialing for cellphone numbers just isn't practical. Also use them in elections such as congressional or state legislative races that involve district boundaries.	Yes	"I try not to weight something up by more than about 15% of what is in-tab. If I'm targeting to get 200 black voters in a sample and end up with 175-180, I'll weight. If I only get 150, I'll selectively oversample. In those circumstances I will use a voter list to target."	Sometimes	"In some states, where voters register by party, data is available showing exactly how many voters in each party are white, black, Hispanic or other. In those states, we keep an eye on it. Most common is making sure Hispanic voters generally reflect their party registration and white vs. black among Democratic voters is relatively in line."		"Not sure what you mean by that. We sometimes conduct panel studies (usually not around elections, though)."	Yes	Yes is too much of an answer. It has happened. I can count on one hand over 25 years. I think the pressure for conformity is a problem in the industry.	Sometimes or Other (please specify)	BCS Computers		Yes	"That is the error rate for responses near 50 percent, but when a response to a specific question is much closer to 0 or 100, the smaller error rate should be noted, depending on the importance of the question asked."				French								Minimal		"We have very little costs on IVR [interactive voice response] -- maybe a few hundred dollars. We own the data and one of the largest robocall systems in the country. For live calls, the cost with cellphones is in the $3,000 range. $2,500 we charge for IVR. $7,000 for live interviews."	N/A			In some way 	Yes		200								Yes			N/A	N/A	Yes	Yes	The raking has changed somewhat. Main issue has been Hispanic non-response requiring a slight further adjustment to age (i.e. weighting up Hispanics to the population proportion tends to overweight young adults -- so the final weighting has to be smoothed a little more).	Yes	We use a call center. The cost is masked to us.	Unknown	Unknown	Unknown	Politely	All live interviews are recorded and can be re-examined in the event of a complaint						52	Same reasons as before -- whatever those were.	"We primarily use voting history. For new voters, we assign voting likelihood based on a number of demographic factors (age, gender, ethnic origin, and 64 others)."	Yes. State board of elections turnout figures. This is for pre-election polls.	Yes	"Vastly richer and more accurate information about past participation and many other useful variables for modeling, etc."	Yes	"I try to stay under 2. I want to stay as close to the actual answers as possible. Weighting is a distortion of the respondents' answers. The more the weight, the further away from what the respondents said."	Sometimes	Pin Voting Rights Act states [http://www.civilrights.org/voting-rights/vra/map.html] with party registration where we have very accurate data.					Sometimes or Other (please specify)	It depends on goal and situation.		Yes	"While margin of error on early election polls have little bearing on final results, we find that it does indeed play its proper role as elections draw near."									Arabic			About 30% to 35%				"We do a statewide mail poll of whatever is on the statewide ballot, whether that's a Senate race, governor, a referendum, whatever. So any cost increases are due to printing and postage increases for 12,000-plus ballots and twice that many envelopes."		The Elway Poll is funded by subscribers.	Yes		14 employees in house (20 employees in the call center)	8	6						Yes			New for us.		Yes	Yes	We added online and mobile.	Yes	"We would have to get that from our vendors. [Followup from one of the vendors: ""Pay is dependent upon geographic location, tenure and performance (ability to deal with difficult respondents, attendance and floor behavior) and varies from $8 per hour up to over $11 per hour. All interviewing staff qualify to receive vacation and sick benefits as well as paid breaks and other company-designed benefits.""]"	"We would have to get that from our vendors. [Followup from one of the vendors: ""The percentage of males (as well as other demographic characteristics) varies by geography as well but overall ranges between 30 and 40%.""]"	"We would have to get that from our vendors. [Followup from one of the vendors: ""The average age varies by center as well, with our Ohio and California call centers trending older (40+ ish). Texas, Florida and Washington trend younger.""]"	"We would have to get that from our vendors. [Followup from one of the vendors: ""All interviewers complete four hours of classroom training followed by at least two hours on a training survey before being able to work on client-sponsored surveys. Additionally, each interviewer is briefed on the specific survey that they are conducting. Briefing includes discussion of the topic, geography involved, special circumstances which may exist among the respondents and pronunciations. No interviewer is allowed to leave the briefing until they have properly pronounced key words to the briefing supervisor.""]"	"We would have to get that from our vendors. [Followup from one of the vendors: ""As part of the initial training, Interviewers are taught to be polite and accommodating during all interactions with respondents. They are taught from the perspective of, 'Imagine if it was you who just was woken up with the call.' If the respondent becomes increasingly abusive, they are instructed to inform their supervisor and to put the phone number on our internal do-not-call list. Numbers on this list are not recalled for 12 months.""]"	We monitor interviews from our office every night.						52				Yes		Yes	"If you weight a subgroup by more than a factor of about 20 percent to no more than 25 percent of the original count of the subgroup, you are creating data that is unreliable. When the subsample is too small to reflect the voting N-size of a particular group, supplement the data by making more phone calls or gather more data by other means, but don't make a mountain out of a molehill of small N-sizes of subgroups, because the higher error rate of the smaller number is carried with that reweighting and seldom noticed by most observers, but it can determine the overall outcome of the poll."	Yes	"Our system combines numerous demographic, psychographic and voting-history data fields into 335 distinct categories. So, yes  we do weight by race and party together, but there are many more data points involved."					Sometimes or Other (please specify)	Trick question?		Yes											None	Not lately.								Yes		3 full-time and 1 part-time on the analysis side -- field is farmed out or hired ad-hoc	3	1	4	0	0	0	"This is a great question. This is a big issue in our field. Few minorities end up choosing quant-related fields in graduate school, so there the pool is small."	Yes				"I don't have this information to hand, and it would take our online team a few days to get me all the details. Panel attrition is an issue, though, across the industry."				Yes		Varies	Varies	3 days of training	"Role-playing, demonstrations, mentoring"	100%						52						Yes	Reasonable  case by case   also do sensitivity analysis to see what difference it makes.							Sometimes or Other (please specify)	Yes and yes.			"It is credible as far as it goes, but it is typically given the one-sentence +/-. Explaining the margin of error and other sources of potential error takes up air-time or column inches that media outlets don't have to spare. Or the patience to explain."										None		Negligible.									"31, 5 full-time staff"	"9, 2 full-time staff"	"22, 3 full-time staff"	"26, 4 full-time staff"	3	1	1										Yes						About 20%						51 or 52	The trendlines on polling on key states have shifted to the GOP in the past couple of weeks.					Yes	Usually between 0.5 and 4.0.											Not sure what that means...										None			"In non-election polls where I have used Spanish interviews, the difference has been negligible."								five	four	one	five	none	none	none										Yes												N/A	N/A					Yes	"When our weighting processes push results too far in either direction, we tend to believe our sample is too far from the norm to trust the results."																					None											varies	about 1/2	about 1/2	all	none	none	none										Yes																		Yes																																								"I don't have the demographic breakdowns readily available, but we are a large organization that is much broader than our political polling unit. We subcontract our telephone surveys. Whether we included or excluded our field houses would affect the demographic profile of our employees."																																																																			I have gone to a network of independent contractors and consultants.																							