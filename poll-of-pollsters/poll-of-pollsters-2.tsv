"Please enter your information, and your polling organization's information. We won't publish your phone number and email address."		Is our rating of your polling organization fair?			Why?	Do any clients or prospective clients refer to our or others' pollster ratings in discussions with you about new business and pricing?		What have they said to you in those conversations?	"Do you think a straight polling average would do better as a Senate forecast than one that adjusts polls for various factors, and incorporates non-polling data, such as FiveThirtyEight's?"		Why do you think so?	Are there any polling organizations whose polls you think we should no longer be including in our forecast?			Why do you think we should no longer be including them?	Do you consult other pollsters' numbers before releasing yours?			"In what circumstances do you do so, and why?"	"Do you adjust your polls' results to match published polls' results, as described in <a href=""http://fivethirtyeight.com/features/are-bad-pollsters-copying-good-pollsters/"" target=""_blank"">this FiveThirtyEight article</a>?"			"If you do sometimes or always do that, under which circumstances, how do you do so, and why? If not, why not?"	"Do you think other pollsters adjust their results to match published polls' results, as described in <a href=""http://fivethirtyeight.com/features/are-bad-pollsters-copying-good-pollsters/"" target=""_blank"">this FiveThirtyEight article</a>?"		"Why do you think so, and if you think some are doing it, which pollsters, and in which races?"	"Is it ever appropriate to <a href=""https://twitter.com/DKElections/status/517890139729973248"" target=""_blank"">recall a poll</a>?"		"Why do you say that, and if you think it is sometimes appropriate, under what circumstances?"	What polling organization other than your own is doing the best work in the country?	Why do you say that?	"If you conduct some polling for campaigns, do you ever let a campaign dictate any of the weighting or other factors that can determine final results?"		"Can you explain why you do or don't, and under what circumstances you would?"	What would you like to see the media do differently in reporting polls?	What question or questions would you want us to ask your fellow pollsters in future rounds of this poll?Name	Polling Organization	Yes	No	Other	Open-Ended Response	Yes	No	Open-Ended Response	Yes	No	Open-Ended Response	Polling organization 1	Polling organization 2	"Polling organization 3, 4, 5, etc."	Open-Ended Response	Yes	No	Sometimes	Open-Ended Response	Yes	No	Sometimes	Open-Ended Response	Yes	No	Open-Ended Response	Yes	No	Open-Ended Response	Open-Ended Response	Open-Ended Response	Yes	No	Open-Ended Response	Open-Ended Response	Open-Ended ResponseShachi Kurl	Angus Reid Global																																			John Anzalone	Anzalone Liszt Grove Research		No		"Polls are put out at a single point in time so I am not even sure how you rate them. If we put a poll out last June, how are you analyzing that? How do you really give that a grade? What are we being compared to?"		No			No	"Polling is just one factor in forecasting. Money, GOTV potential, minority makeup of the state all needs to be factored in. For example, minority support is never fully solidified in polls but should be extrapolated out to Election Day probability in forecasting."				"I will leave you to be the polling police. We see a lot of bad polls out there. You never know how they are pulling a sample, their call-back procedures, etc. Many times the poll is done in too short of time and the demographics do not reflect voter history, especially in midterms. I still don't understand why polls that are done for media outlets are not exclusively doing likely-voters screens. Voter files nowadays have great voter history. It is just not that difficult to build a voter model for elections. Although the GOP had a tough time doing it in 2012, as did Gallup."			Sometimes	"We keep tabs on the averages of statewide polls from FiveThirtyEight, Huffington Post, Real Clear Politics, etc. because they are helpful in giving you a check. More than anything, it tells you if you might have a bad dataset which of course happens nearly 5% of the time. We don't manipulate our data because of other public polling but we do look at the averages."		No		"No, we trust our data. At times if we think we have an outlier, we will go back into the field at our own cost to check out numbers. Again, you are going to get bad data sets. That is just statistics."			I have no idea.	Yes		"Yes, but you should redo before you make something public. If you think you have a bad data set, go back in and check it out. Nothing wrong with that. You are going to get bad data sets 5 out of 100 polls. The difficulty is figuring out which 5."	"Pew Research Center in public polls, Michael Bocian in Democratic politics"	Pew has become the gold standard of polling.		No	"However we work with the campaign to make sure, especially in a midterm, that the voter model is accurate. We have our own equations about how much of a sample is from 2010 voter history, new voters registered after 2010 who voted in 2012 and new registrants from 2012."	Stop publishing shitty polls.	I am just trying to survive the next friggin' three weeks.Gabriel Joseph	ccAdvertising		No		We do not know how you judged what was right and wrong. The elections have not happened yet. The only survey that matters is what happens on Election Day.		No	The only survey that counts is the one that happens on Election Day.	Yes		"More data leads to better results. Looking backwards at 2012 does no good in 2014. If someone said that they had a great idea and wanted you to invest $1 million in it but there was one catch, the data they were using was two years old, no one would invest in it. Data today is so much different than data in 2012. This includes voting patterns and turnout models. The same is true for 2010."						No		We do surveys differently.		No					Do not know.	Yes			Public Policy Polling	The last election showed that ccAdvertising and Public Policy Polling were the most accurate pollsters. The only poll that counts is the one that happens on Election Day.		No			What percentage of your polls are mobile phones?Kjersten Adams	Dan Jones & Associates																																			Stuart Elway	Elway Research	Yes			I think it accurately reflected what you measured.		No	"Most of the feedback has been from existing or previous clients, who were congratulatory. It has not yet come up in any new business discussions."		No	Your record has been very good -- indicating that your formula is sound. And there are differences in survey firms that should be taken into account.							Sometimes	"If there are numbers available, I take note of them. I don't do anything as a result -- I just want to anticipate any ""kill the messenger"" reaction if the results are divergent."		No							No	"I don't think a published poll can be ""recalled."" What does that even mean? A second poll can be done, with appropriate modification of methods, if necessary. But I don't see how the first poll can really be recalled."							Bernie Porn	EPIC-MRA						No			No	History suggests that polling data alone does not always predict election outcomes.						No		"Since we do most of our election polling for our media clients, we keep our polling results absolutely confidential until survey results are published by our clients. Moreover, our media clients do not discuss results with candidates until they are published, with comments from candidates coming in follow-up stories."		No		We never adjust our results to match any other polling organization because we are paid to provide our findings and not to be influenced by others.										No	"If we do the polling for a political or issue campaign, they can decide if they want to release the data, but they can't dictate what those results are."	"If a pollster is unwilling to provide the wording, order and placement of the questions asked, as a general rule, the media should not report on the results of such polls. We always provide the question wording and placement order. If the media is reporting on crosstab results, they should be more cautious about reporting small sample size."	Mark DiCamillo	Field Research Corporation (Field Poll)	Yes			We've had a lot of practice tracking voting preferences in California -- this year is the poll's 67th year. Previous academic assessments of the correlation between our final pre-election polls and election outcomes over the years have also shown similar results.		No			No	"Historical factors, situational variables unique to an upcoming election and the track record of the pre-election polls in each state are likely to improve upon forecasts so long as they are done in a nonpartisan manner."					Yes			"I follow all the statewide polls in California that I can get my hands on, in both election and non-election years."		No			Yes		No comment	Yes		"If some inadvertent error was found in the poll after its release, then the pollster should own up to this."	Pew Research Center	"I felt their very large-scale final pre-election nationwide survey released immediately prior to the 2012 election provided us with a clearer picture of the preferences of voters in that election than any other poll, including the exit polls. I also find their polls documenting the very modest differences in poll results between polls with typical responses rates of 9% or so to those with much higher response rates to be of great value."			N/A	The declining number of seasoned political reporters covering California politics has on occasion led to misinterpretations of our poll results or a lack of understanding of what we believed were significant findings or implications from our polls.	To the traditional telephone pollsters using probability-based sampling techniques: Is it better practice to stratify your samples by age to ensure a proper representation of adults of all ages than to attempt to correct age-related imbalances in your surveys through weighting?Berwood Yost	Franklin & Marshall College			Other	"Attempting to classify pollsters is a good idea, but I'm not in complete agreement with your approach. I published my concerns after the 2010 ratings on Pollster. [http://www.huffingtonpost.com/guest-pollster/yost_borick_the_silver_standar_b_727850.html]"		No				"It depends when your forecasts are published. Many months prior to Election Day, the non-polling factors are important, but in past elections the polls and the predictions converged as Election Day approached. I think the correlation between simple poll averages and the FiveThirtyEight outcome predicted in each state in 2012 was around 0.99. So, I think that generally the full models are better over the long scope of an election season, but become less important as Election Day nears. That will hold as long as polls continue to perform well."				I don't believe party-affiliated polling firms should be included in the models. I have a concern that partisans are increasingly producing polls in the hope that they can affect the poll averages in a race. I think we see direct evidence of that in this year's Pennsylvania gubernatorial contest.				"What do you mean by consult? Given the attention to polling, it is impossible not to know what the other polls in major races are reporting. So, I do know what the other polls show in the races in which I poll. I do not, however, make any adjustments to my data based on those averages. My weighting is based on selection probabilities and non-response using data from our sample files."		No			Yes		"The evidence presented in the FiveThirtyEight article is suggestive, but I don't have any direct evidence."	Yes		"There may be circumstances where this is called for, so I won't say never. We've never done so, but I think if you've produced results that appear biased on variables of importance, one might at least consider not releasing the poll. But those decisions should be made prior to actually releasing the results."		"There are many organizations that do good work. I think doing good work means a lot more than just producing estimates about the status of one race or another. Good polling provides essential context for understanding the contours of a race. Polling firms that fail to provide that context are doing a poor job. That is one of my other concerns about the pollster rankings -- they don't really reward those firms who take the time to explain the underlying fundamentals and dynamics of a race. For me, explaining those things is more important than the horse-race numbers."			We don't do any polling for candidates.	Distinguish between the good and the bad. Most media treat all polls equally. Some polls are so biased that they shouldn't be reported.	Doug Kaplan	Gravis Marketing	Yes			We believe the rating is fair. The majority of our publicly released polls are conducted by IVR [Interactive Voice Response]. This is our first full election cycle; we believe the rating will improve.	Yes		We have pointed out that we have released many polls since 2012; for a company starting out our results are reasonable. In many races we are very accurate.	Yes		"FiveThirtyEight is revolutionary. With the amount of undecideds, how and when it was conducted, incorporating various factors make sense for an average."				"I am not sure how you can determine this. Partisans are always going to disagree with certain polling organizations. As long as the polls are conducted, they should be included."						No						Yes		"Since we have been polling, once in 2012 in Colorado, and once in 2014 in Wisconsin. Sometimes mistakes are made. Companies should be transparent. If they feel a mistake has occurred they are obligated to conduct a second poll."				No	We have a research team that builds a turnout model on each race that we poll.	The media seems to always give the benefit of the doubt to pollsters they believe are liberal or associated with the Democratic Party. Companies they consider conservative seem to be discounted and treated much differently when they make a mistake.	Matt Towery	InsiderAdvantage		No		"We poll a large number of races across the southeastern U.S., where changing demographics, such as race, play heavily in cross-weighting and make accurate predictions far more difficult. A firm that polls Iowa, and knows Iowa, would likely have a greater chance of earning an ""A"" than a firm that aggressively polls both primaries and general election contests in the South. Without a variable measuring the difficulty of races, the results will inevitably favor firms outside of regions with shifting demographics and political affiliations."										"No; we consider those who objectively attempt to poll and who have done so for numerous years to be worthy of any forecast. We would consider any objections to other pollsters to likely be the result of a lack of understanding or bias against that firm, and again, highly damaging."		No				No		"Obviously, we maintain objective results, irrespective of other pollsters."													Julia Clark	Ipsos									No	Because we've done the calculations ourselves and know adjusted models work better.				"There are none we think should be excluded. More data is better, as a general rule."	Yes			"We do it to anticipate potential questions or backlash, and to help our clients plan for this. It would be crazy not to look at where the market is before publication!"		No				No	"This question is too simplistic. I don't think pollsters adjust to match someone else's published results -- that would of course fly in the face of any kind of good practice, and would become obvious very quickly (and discredited). But with the renewed focus on Bayesian statistics (http://www.nytimes.com/2014/09/30/science/the-odds-continually-updated.html?_r=3) it is inevitable that pollsters will begin to take cues from the marketplace. The form this takes will vary enormously by pollster."	Yes		"The question is sort of moot; once a poll is published, it is out there in the public domain. It cannot be truly ""recalled."" But if an error is discovered, certainly a company should say so and indicate the problem and their solution. There is so much emphasis on speed these days that errors inevitably creep in... it is important to acknowledge them. If that takes the form of a ""recall,"" so be it."	Many	Companies that are innovating to cope with the increasing constraints on traditional polling models take risks by pushing forward the market in ways that make some people uncomfortable. But this is critical for our industry to adapt and grow. The proof is in the pudding: Who gets it right consistently? Lots of companies have good and excellent records when it comes to polling.			N/A -- we do not do any polling for campaigns (or parties or politicians)	"I am under no illusions that I (or anyone) can influence the way the media reports polls. It is dictated in part by political pressures from campaigns as well, whose job it is to attempt to discredit any poll that detracts from their candidate. But some open-mindedness on methodologies would be welcome, especially as the inevitable shift away from traditional polling approaches."	Barbara Carvalho	Marist College			Other	"Your model is a thoughtful and honest attempt; it just places too much weight on making sure there are enough cases rather than other criteria. Concerns: 1 - Regarding the years you cover, 1998 to present, it may be useful to see if there are any changes from election cycle to election cycle rather than the long-term view. I recognize your approach provides many more cases for organizations that don't poll often, but it can obscure recent changes that organizations take in a rapidly changing polling environment. For example, The Marist Poll has changed from a cutoff model to a probability model in assessing turnout and likelihood to vote starting in 2012, not to mention the change in the proportion of cellphone sample. 2 - By focusing on the margins separating the two top candidates, it raises two issues: First, if candidate A wins by 5 points, is a poll which has candidate A with 15% and candidate B with 10% with the remainder undecided or for another candidate better than a poll that has candidate A at 46% and candidate B at 40%? Second, if a poll is closer on the margin but has the wrong winner, that poll is touting poll watchers in the wrong direction editorially despite the closer stats. This runs counter to the main goal of public opinion polls. 3 - In an environment where turnout is low and the electorate is highly polarized, voter volatility is reduced. The conclusion, however, that the final three weeks don't really matter much suggests that this will always be the case over time and that campaigns are misguided in spending on GOTV, early vote, and other campaign tactics and targeting. 4 - Campaigns don't stop when polls are conducted. A shorter window before Election Day makes more sense, realizing this would limit the number of cases you would have. With The Marist Poll's media partners, we do not pick the timetable for our polls. In fact, few media, understandably, want to release an Election Eve poll on Election Day which, as you point out, is the easiest to gauge an election outcome. We generally poll before and after campaign events (debates) and to characterize the races and electorate prior to Election Day. Rating the polls is not novel. NCPP [National Council on Public Polls], as you may know, did this after each election cycle. [http://www.ncpp.org/?q=node/135] Not a perfect approach, but it provided a balanced assessment in its simplicity."		No		Yes		"I appreciate what the models are trying to accomplish but it appears that they are also adding assumptions that may work well in some settings (cycles) and not others. Of course, the difficulty now with the straight polling average is the growing number of organizations that are trying to influence that average."				I'm not familiar with each of the over 300 polling organizations that are in your model.		No				No				No	I think that there may be use of the internals of some public polls to inform sample balancing and weighting of others.		No	The results are the results.	NBC News/Wall Street Journal	Their focus on characterizing the electorate and greater attention to issues rather than just tossup. Strong polling methods and excellent analysis.		No	The Marist Poll does not do any campaign polling and never has weighting dictated by anyone else.	"Many in the media do a good job analyzing the polls, particularly the polls that provide information beyond the horse race. It's important to discuss content beyond just the horse-race trends."	Brad Coker	"Mason-Dixon Polling & Research, Inc."														I would never tell you who and who not to include. It's your business and your reputation on the line. You need to do what you feel is in your best interest.																		No	Never. My job with any client is to tell them what they need to know and not what they want to hear.	"Be more wary of the ""free stuff"" that shows up in their inbox."	Nothing in particular.Seth Rosenthal	Merriman River Group									No	Some polls and polling organizations are better than others and should be weighted accordingly. And non-response (as well as other types of bias) in a particular election or cycle is often correlated regardless of pollster. So incorporating other objective factors that have a proven track record makes sense as well.																									Steve Mitchell	Mitchell Research & Communications																No				No						Yes		"It is in the best interest of every pollster to work as hard as possible to get the correct results. However, if a pollster makes a serious mistake and realizes it after the fact, then it should explain the error, redo the poll, and issue the new results based on correcting the error. A poll cannot really be recalled, but mistakes can be rectified in a new poll."	J. Ann Selzer	Her company has an outstanding track record and she is almost always correct.		No			Patrick Murray	Monmouth University																																			Christopher Borick	Muhlenberg College	Yes			"Measure of accuracy has strengths and weaknesses based upon the criteria included and not included. In general, I think the FiveThirtyEight ratings were valid measures of poll performance."		No		Yes		This would be complex and of course the adjustments would have to be very transparent and based upon solid methods. Giving equal weight to polls with unproven or poorly performing methodologies reduces the usefulness of the average.				"I won't name polls that I think should be excluded, but if there are firms that do not meet performance or transparency standards, it is in the best interest of the forecasting model to exclude those polls."			Sometimes	"We of course check other polls that are working on the races we are polling. When we release a poll, the media will inquire about our results in relations to others, so it is important to have a sense of where our results stand."		No		"If this was done, it would completely reduce the independent nature of the sample that was employed. Our results reflect the responses from the sample selected and any weighting to population parameters that we feel are appropriate."	Yes		I imagine this is done by certain pollsters and I believe that FiveThirtyEight has found some evidence that some polls produce results that tend to move more towards the averages than others as elections get closer. [http://fivethirtyeight.com/features/are-bad-pollsters-copying-good-pollsters/]			"I'm not very clear on what you mean by recalling a poll. Would the recall be by the firm releasing the poll? I imagine if we discovered an error in our methodology or reporting that we would release a correction. If the methods flaw was so significant that it undermined the results, I imagine a full recall would be called for."	There are many great polling organizations that are out there doing wonderful work. I believe their methods and performance over the years speaks to the quality of their work.			No	We don't work with campaigns.	Spend more time discussing methods and transparency. It still seems that all polls are treated equally in a great deal of media coverage and that confuses the public and ultimately undermines trust in public-opinion research in general.	Scott Keeter	Pew Research Center																																			J. Ann Selzer	Selzer & Company	Yes			"You did not contact us for dates, data -- anything. You found what was published. No one would monkey with the findings -- they were completely data-driven."					No	"Straight polling averages abound. They seem to suggest that if you just average all the data out there, it gives you the right answer. Given what Nate has written about conformity in poll results, what looks to be an outlier could, in fact, be the most accurate. You take into account that all polls are not created equal. I think that helps. Oh, and your track record is exceptional."																							"I do not do work for candidates. But, I have this comment. A campaign poll often has a different job to do that a public poll. A long time ago, I worked on a referendum in a special election. Really, who knew who would turn out for that? So one of the clients offered ideas of how to sample, based on exceptional knowledge of these matters. OK. We were not planning to release the poll findings, so if taking a look at a couple of groups helped decide where to spend money turning out the vote, that's useful. That kind of poll is designed as a pointing device to the best prospects. It is not designed necessarily to capture an accurate picture of the way the vote would go, if held today. So long as everyone agrees, that's fine. Now, if a campaign tried to publish a poll I did not think reflected an accurate picture, yet they wanted to portray it that way, I reserve the right to comment and clarify. Publicly. Or take my name off the poll, I suppose."	"I would like to see specifically how the pollster is defining ""likely voters."" It is very hard to evaluate a poll if you do not know the technical definition. They do not have to reveal secret sauce, but they need to tell me what factors they used in defining a voter as likely to vote, and if they gave some likely voters more weight in their polls than others."	Don Levy	Siena College	Yes																																		Jay H. Leve	SurveyUSA																																			Darrel Rowland	The Columbus Dispatch		No		"To get an average error rate that high, you must be including a significant outlier from the 2006 general election, where our poll accurately reflected the outcomes but the margins were overstated. In 2012, this very site ranked our poll tops in the nation as far as a single-state poll's accuracy on the presidential election, after we matched Ohio's election-night totals. In 2010, our poll of the governor's race matched the actual outcome to 0.1 percent. In 2008, we were just 1 point off Obama's and McCain's totals in Ohio. Plus we are one of the few polling organizations in the country to even try to poll less-predictable downticket races, including Supreme Court seats. Still, the poll has been very reliable in those races, including an accurate foreshadowing of an incumbent justice losing in 2012. Also, I don't know if you are ""marking us down"" for not using cellphones - since we are that odd beast of a mail poll, cellphone users have exactly the same odds of being sampled as all others."		No	N/A		No	Not even close to mathematically or statistically valid.				I seriously question the validity of any survey relying on robocalls.		No		Why would I?		No		N/A			I simply don't know.	Yes		If serious methodological errors are discovered.		Insufficient data		No	We never do this.	"Forget robo-polls and those sponsored by someone with a dog in the race. Pay attention to the margin of sampling error, but be very wary of the ""statistical tie"" verbiage that has become so common. If an organization won't let you see not only their full methodology but also all the questions and their order, be very suspicious."	"How do you determine a ""likely voter""?"Andrew Smith	University of New Hampshire		No		"You don't seem to take into account AAPOR best practices for methodology used and rely more on final election predictions, which are quite accurate regardless of methodology, questions, etc used."					No	"It depends on the methods used by the polls you include in your averages. There is a Gresham's Law operating in state polls where the proliferation of cheap polls, but of dubious methodology, drives out well-constructed polls using AAPOR best practices."	Any IVR [Interactive Voice Response] poll	Any web poll		"IVR [Interactive Voice Response] polls have atrocious response rates. Web polls do not have sampling frames that allow selection of random samples. Both reflect a level of self-selection that makes them convenience samples. In spite of declining response rates for live-interviewer telephone polls, their methods are far superior to IVR or web surveys."		No		"Trust your methods, trust your numbers."		No					I hope not.	Yes		For serious data-collection problems -- if CATI [Computer Assisted Telephone Interviewing] was programmed wrong or if the sample had incorrect geography.					We never conduct polls for campaigns.	"Focus on the inattention that most people have toward elections. Read ""The Illusion of Public Opinion"" by George F. Bishop and ""The Opinion Makers"" by David W. Moore."	Gregg Durham	We Ask America																													CBS News/New York Times/YouGov	We like their overall approach and they aren't afraid to disagree with others.					