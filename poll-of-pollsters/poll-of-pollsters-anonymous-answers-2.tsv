Is our rating of your polling organization fair?			Why?	Do any clients or prospective clients refer to our or others' pollster ratings in discussions with you about new business and pricing?		What have they said to you in those conversations?	"Do you think a straight polling average would do better as a Senate forecast than one that adjusts polls for various factors, and incorporates non-polling data, such as FiveThirtyEight's?"		Why do you think so?	Are there any polling organizations whose polls you think we should no longer be including in our forecast?			Why do you think we should no longer be including them?	Do you consult other pollsters' numbers before releasing yours?			"In what circumstances do you do so, and why?"	"Do you adjust your polls' results to match published polls' results, as described in <a href=""http://fivethirtyeight.com/features/are-bad-pollsters-copying-good-pollsters/"" target=""_blank"">this FiveThirtyEight article</a>?"			"If you do sometimes or always do that, under which circumstances, how do you do so, and why? If not, why not?"	"Do you think other pollsters adjust their results to match published polls' results, as described in <a href=""http://fivethirtyeight.com/features/are-bad-pollsters-copying-good-pollsters/"" target=""_blank"">this FiveThirtyEight article</a>?"		"Why do you think so, and if you think some are doing it, which pollsters, and in which races?"	"Is it ever appropriate to <a href=""https://twitter.com/DKElections/status/517890139729973248"" target=""_blank"">recall a poll</a>?"		"Why do you say that, and if you think it is sometimes appropriate, under what circumstances?"	What polling organization other than your own is doing the best work in the country?	Why do you say that?	"If you conduct some polling for campaigns, do you ever let a campaign dictate any of the weighting or other factors that can determine final results?"		"Can you explain why you do or don't, and under what circumstances you would?"	What would you like to see the media do differently in reporting polls?	What question or questions would you want us to ask your fellow pollsters in future rounds of this poll?Yes	No	Other	Open-Ended Response	Yes	No	Open-Ended Response	Yes	No	Open-Ended Response	Polling organization 1	Polling organization 2	"Polling organization 3, 4, 5, etc."	Open-Ended Response	Yes	No	Sometimes	Open-Ended Response	Yes	No	Sometimes	Open-Ended Response	Yes	No	Open-Ended Response	Yes	No	Open-Ended Response	Open-Ended Response	Open-Ended Response	Yes	No	Open-Ended Response	Open-Ended Response	Open-Ended ResponseYes			Because the standards and criteria used seemed reasonable.	Yes		Congratulations!	Yes		"Neither yes nor no on this, but I picked yes because sometimes you can overthink this stuff. Sometimes straight averages are just as good. In many cases you are arguing that one aggregator is better than another because they got the result closer by a couple of tenths of a point. Not a big deal."				No -- that's for you guys to decide!	Yes			"If by ""consult,"" you mean ""look at,"" then yes, of course we look at them."		No		"There have been a few times where our results have been the only one right...and times where our results were flat-out wrong. We may be wrong, but we'll live with that rather than being dishonest."	Yes		I am not sure; I think everyone looks in case of an outlier.	Yes		"Because pollsters are under pressure to turn data around very quickly, I can imagine a circumstance where something odd happened in the data and no one caught it. Maybe the wrong weight was applied. Maybe a base was not properly set. If a pollster no longer thinks the poll is accurate -- AND can identify a methodological reason why, sure. Recall it and adjust. Hard to send it back out. If more pollsters were willing, there might be less pressure for conformity."	Ciruli Associates	"Never heard of the guy, but he (along with the Field Poll) were awarded A+. Obviously, I put some stock in your analysis and ratings."		No	"I don't do polling for partisan campaigns. There is often tension between clients who are by definition advocates and researchers over the design of a survey. The final design depends on the objective of the survey -- how the data are going to be used. If it is purely for internal strategic purposes, there is more leeway and there may be reasons for various design elements. If it is to be published, the methods need to conform to accepted standards."	"A surprising number of ""reporters"" who write about polls, when all is said and done, know little about polling. I'm not talking about a general-assignment reporter in Biloxi who may write up one poll every four years. I'm talking about some of the nation's most pre-eminent, self-appointed polling experts."	Do you find the models of the polls helpful or distracting?Yes				Yes		"They are sympathetic and consider us to be accurate, but we have been affected with regard to business and our reputation. It has been damaging to us outside of our own state."	Yes		The aggregated percentages at RealClearPolitics.com predicted 48 of 50 presidential outcomes successfully.					Yes					No		"Um, you think anyone is going to answer this honestly?"	Yes		I've just noticed over the years that certain organizations have wild fluctuations in their numbers that often follow the release of a poll with numbers that run counter to what they had recently published.	Yes		"If there is a legitimate instance of methodological errors, etc., then it is appropriate for a firm to recall a poll."	Gallup	"Gallup is consistently accurate as to presidential approval, which provides a ""constant,"" which all other pollsters can look to in determining the validity of their efforts."		No	"I wouldn't say we let campaigns ""dictate"" anything. We will run outcome tests for campaigns under different, sensible turnout conditions as requested. But we also make it clear what the results are using our best, most objective, turnout scenario."	"An increasing number of reporters are looking at polls as easy news; results are released, they cherry-pick the results that make the best copy and exploit it."	Do you have off-the-record conversations with other pollsters to compare results?	No		"You say we don't call cellphones, yet we do. We use live operators to introduce the automated questions after permission is granted."	Yes		"They follow FiveThirtyEight.com and are impressed to see us in, and doing well in, the ratings."		No	I think you're doing it right.						No		"I am aware of others' numbers but I do not consider them as we poll, or weight."		No			Yes		"If you look at all the swing states and the average of the final polls in 2012, the presidential contests were 1-4 points closer than the final result. I do not know for sure that anything was happening. But your question asks whether I THINK it happens. I think some feel safe when their numbers match others."	Yes		"If we have discovered a methodological error in our work, it is incumbent upon us to recall/correct the record/walk it back."	Gallup			No	We don't work for any campaigns.	"It would be better public understanding if major media outlets that still conduct their own polls (e.g., CBS News/New York Times, NBC News/Wall Street Journal) were a little more humble about reporting their own results as gospel and instead provided more context using other high-quality polls that have been conducted on the same issues or elections."	Do you weight by race and party? Example: weighing African American Democrats instead of African Americans as a whole. Do they always disclose who is funding the poll?		Other	"Based on our small number of public polls since 2010, the rating is essentially fair. However, there are three issues that you do not (two of which you cannot) take into consideration. First, most of our polling is done for private political clients in very local races (elections no national media covers). Within this niche, we have been very successful and accurate. You obviously can't consider these in your ratings (because they're private even more than because they're local), which is simply unfortunate for us. Second, since 2010, we have made two major (and many minor) adjustments to our methodology to reduce the use of subjective ""political judgment"" (particularly regarding sampling and weighting) in favor of more objective alternatives. Finally, there is the issue of incorporating/proportioning ""undecideds"" in poll results (particularly blowouts), given that ratings of polling results tend to focus on the difference between the toplines for the two candidates without regard to apportioning the percent undecided. For instance, if we have a poll that is 62%-18%, with 20% undecided (most pollsters either won't poll, or won't report races like this, but we do occasionally), we do not expect the undecideds to break evenly (unless something in the internals suggests they will). We generally expect them to break much more closely to the ratio of decideds (mostly, but not entirely, because many among those 20% undecided most likely aren't going to vote for that office). So, in a case like this, while the topline suggests a 44% margin, we would likely predict a bigger blowout, closer to 56%. Our work in local polling, where there are often many blank votes down-ballot, generally bears this approach out. It is unfortunate to encounter situations where results like these are judged to be a huge miss, when in fact, they're very accurate. In another instance, the first poll we published publicly as a team was in the Scott Brown/Martha Coakley Senate race. In that one, the topline looked bad for us (we had Brown +10%, and he won by about 5%). But in the internals, 90% of our undecideds had a positive view of President Obama, and many identified as Democrats and/or Liberal, and thus, in the article about the poll, we said directly that Brown's topline was unlikely to increase, and Coakley would probably bring the vast majority of the undecideds. This is exactly what happened, and thus our forecast based on the entire poll (rather than just the head-to-head) was pretty much on the mark (but not likely what would be considered for your ratings). I could go on, but won't! Anyhow, in our media reports, we nearly always go out on this limb to suggest how the undecided numbers are likely to break. Since we're not in the business of actually proportioning our undecideds in the reported toplines (and aren't going to change that just to get better ratings), it would be nice to get credit for understanding how this works and getting it right (or losing credit when we get it wrong). We may encounter a situation like this in our upcoming Hawaii Senate and/or CD2 polls for Civil Beat, and will comment on it in the articles as needed."	Yes				No	Too many mysterious polls being released. I think some are being financed by partisan groups without disclosure. A straight average would be skewed simply by one side putting out more polls than the other.						No		"The word there is ""consult"" and what I think you mean by that. Obviously, I would have to be aware of what the other polls have said. I can't poll a race if I'm living as a hermit. If I am far off from the range of other results, I may take a second look at my sample demographics before releasing to see if it meets my expectations. But I don't benchmark my sample to others."		No			Yes		"It is unlikely that ""no"" pollsters adjust their numbers, and I have seen results in the past that I felt were likely adjusted, but none recently."	Yes		Mistakes happen; the best remedy is to own up to it.	Not sure	I don't pay much attention to what others do.		No	We prefer to not poll for campaigns.	"The print media in my state do a pretty good job of explaining polls. Interestingly, now that most poll results are reported in their blogs, they seem to have more time to explain them. TV is a different matter -- they don't have/take time for anything but one topline result, maybe the most striking crosstab, and a margin of error -- if that."	"Have you modified any of your methods or practices in response to comments and/or ratings from a polling-aggregation site? If so, why and how?"		Other	Can't say.		No	"Our business -- other than our main media client -- is not polling or political work, so clients do not focus on this."		No							No		"Well, I'm generally aware, but I do not automatically check before we publish. With our method, we just take our best shot. I don't really want to care what other polls say."		No			Yes			Yes		The impact of an unforeseen event while in the field could justify not publishing a poll.	Pew Research Center	They are independent and nonpartisan.		No		"Treat equally the views of polling firms whose genesis comes from a Republican background, versus a Democratic. Additionally, show value to small independent pollsters who do not have the media coverage of larger firms, but who may well contribute substantially to determining shifts or trends in a given political race or issue."	How are you determining the weight of party affiliation?		Other	Don't know what it is.		No			No							No				No				No		Yes		"We have not, and would not, simply squelch a poll result. But we have recalled and rerun polls twice, at the request of media clients, when the results indicated a short-term 20+% shift away from a favored, well-known candidate to an essentially unknown candidate. In both cases, we did not have methodological concern ourselves, but our media clients wanted to be sure before they went to press. And in both cases, the second poll confirmed the initial result. From our perspective, in cases where there are sampling concerns, we believe it is best to collect more data as needed to address the sampling issue rather than to recall a poll."	"Pew Research Center, non-election polling"	"Unlimited budget, unlimited resources, no time constraints."		No		Where do we start?! Media needs to be more alive to the long-term records of pollsters instead of consuming every poll like crack.	"If reweighting is done on a poll, what are the limits used in reweighting in terms of increasing the N-size of a demographic breakout?"		Other	I don't know which polls have been included in the rating.		No			No								Sometimes	"On occasion, especially if I see an outlier. We always look for input and have developed strong relationships with a few other companies."		No				No		Yes		"Yes -- if an error is discovered after the fact it is the right thing to do. However, it happened to me once, and admitting the error ahead of the election created a firestorm. In hindsight, there is a part of me that feels I should have just let the bad numbers stand and after the fact blamed it on margin of error or the 5% probability that any poll can be wrong. Would have made my life easier."	Selzer & Company	"They're thoughtful, conscientious, and accurate."			Under no circumstances do we conduct political polling for campaigns.	"Yes, more than just horse race, understanding margin of error, explore the nuance."	None		Other	No one likes a C+!		No											Sometimes	They have been published and are part of the public record and we are alive to what is happening in the world of polling.		No				No		Yes			Stanley Greenberg	"I have known him for many years, and while he has not always been perfect in all of his national findings, his message testing, data interpretation and ability to explain his findings always seems to be a few cuts above most other national pollsters. Also, while his shop is a Democratic polling firm, he has always been willing to say when he feels Democrats and President Obama are getting messages wrong, and why."							Other	"Yes, but they are so close together that the grading cuts are arbitrary. I acknowledge that Nate noted this. I think one of the issues is going back so far in time. Personnel changes (particularly at university shops) can mean an entirely different approach (which may be significantly better or worse than the prior staff). Also, some of the larger shops may have responded better to the challenges of the last few years than others. I think it would be interesting to see these ratings based on the last five years and last 10 years as a supplemental list. I think that time horizon would be more useful in some respects."		No											Sometimes	"We do read other pollsters' results all the time, but it is not a deciding factor on whether or not we release our results."						No			No	"As a general rule, polls should not be recalled, unless some event makes the results useless, but even then it is probably better to release the results and qualify those results based on the event."	Wouldn't know											No																			I really don't know.															No																			I would have to say don't know.															No																													